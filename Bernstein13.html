<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Opportunistic Strategies for Generalized No-Regret Problems | COLT 2013 | JMLR W&amp;CP</title>

		<!-- Stylesheet -->
		<link rel="stylesheet" type="text/css" href="../css/jmlr.css">

		<!-- MathJax -->
		<script type="text/x-mathjax-config">
  			MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
		</script>
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>

		<!-- Metadata -->
		<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Opportunistic Strategies for Generalized No-Regret Problems">
<meta name="citation_author" content="Bernstein, Andrey">
<meta name="citation_author" content="Mannor, Shie">
<meta name="citation_author" content="Shimkin, Nahum">

<meta name="citation_publication_date" content="2013">
<meta name="citation_conference_title" content="Conference on Learning Theory">
<meta name="citation_firstpage" content="158">
<meta name="citation_lastpage" content="171">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v30/Bernstein13.pdf">


    </head>
    <body>

	<div id="fixed">
<a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="../img/jmlr.jpg" align="right" border="0"></a> 
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers
 </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> 
News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> 
Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a>
 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> 
Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open 
Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="../img/RSS.gif" class="rss" alt="RSS Feed">
</a>

</p>
	</div>

	<div id="content">
		<h1>Opportunistic Strategies for Generalized No-Regret Problems</h1>

<div id="authors">Andrey Bernstein, Shie Mannor, Nahum Shimkin</div>;
<div id="info">JMLR W&amp;CP 30 : 158–171, 2013</div>



<h2>Abstract</h2>
<div id="abstract">
	No-regret algorithms has played a key role in on-line learning and prediction problems. In this paper, we focus on a generalized no-regret problem with vector-valued rewards, defined in terms of a desired reward set of the agent. For each mixed action q of the opponent, the agent has a set <span class="math">\(R(q)\)</span> where the average reward should reside. In addition, the agent has a response mixed action p which brings the expected reward under these two actions, <span class="math">\(r(p, q)\)</span>, to <span class="math">\(R(q)\)</span>. If a strategy of the agent ensures that the average reward converges to <span class="math">\(R(\bar{q}_n)\)</span>, where <span class="math">\(\bar{q}_n\)</span> is the empirical distribution of the opponent’s actions, for any strategy of the opponent, we say that it is a no-regret strategy with respect to <span class="math">\(R(q)\)</span>. The standard no-regret problem is obtained as a special case for scalar rewards and <span class="math">\(R(q)\)</span> = <span class="math">\({r \in R: r \geq r(q)}\)</span>, where <span class="math">\(r(q) = \max_p r(p, q)\)</span>. For this problem, the multifunction <span class="math">\(R(q)\)</span> is convex, and no-regret strategies can be devised. Our main interest in this paper is in cases where this convexity property does not hold. The best that can be guaranteed in general then is the convergence of the average reward to <span class="math">\(R^c(\bar{q}_n)\)</span>, the convex hull of <span class="math">\(R(\bar{q}_n)\)</span>. However, as the game unfolds, it may turn out that the opponent’s choices of actions are limited in some way. If these restrictions were known in advance, the agent could possibly ensure convergence of the average reward to some desired subset of <span class="math">\(R^c(\bar{q}_n)\)</span>, or even approach <span class="math">\(R(\bar{q}_n)\)</span> itself. We formulate appropriate goals for opportunistic no-regret strategies, in the sense that they may exploit such limitations on the opponent’s action sequence in an on-line manner, without knowing them beforehand. As the main technical tool, we propose a class of approachability algorithms that rely on a calibrated forecast of the opponent’s actions, which are opportunistic in the above mentioned sense. As an application, we consider the online no-regret problem with average cost constraints, introduced in Mannor, Tsitsiklis, and Yu (2009), where the best-response-in-hindsight is not generally attainable, but only its convex relaxation. Our proposed algorithm applied to this problem does attain the best-response-in-hindsight if the opponent’s play happens to be stationary (either in terms of its mixed actions, or the empirical frequencies of its pure actions).
</div>

<h2>Related Material</h2>
<div id="extras">
	<ul>
		<li><a href="Bernstein13.pdf">Download PDF</a></li>
		
	</ul>
</div>

	</div>

    </body>
</html>
