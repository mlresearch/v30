---
pdf: http://proceedings.mlr.press/v30/Shamir13/Shamir13.pdf
title: On the Complexity of Bandit and Derivative-Free Stochastic Convex Optimization
abstract: The problem of stochastic convex optimization with bandit feedback (in the
  learning community) or without knowledge of gradients (in the optimization community)
  has received much attention in recent years, in the form of algorithms and performance
  upper bounds. However, much less is known about the inherent complexity of these
  problems, and there are few lower bounds in the literature, especially for nonlinear
  functions. In this paper, we investigate the attainable error/regret in the bandit
  and derivative-free settings, as a function of the dimension d and the available
  number of queries T. We provide a precise characterization of the attainable performance
  for strongly-convex and smooth functions, which also imply a non-trivial lower bound
  for more general problems. Moreover, we prove that in both the bandit and derivative-free
  setting, the required number of queries must scale at least quadratically with the
  dimension. Finally, we show that on the natural class of quadratic functions, it
  is possible to obtain a “fast” O(1/T) error rate in terms of T, under mild assumptions,
  even without having access to gradients. To the best of our knowledge, this is the
  first such rate in a derivative-free stochastic setting, and holds despite previous
  results which seem to imply the contrary.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Shamir13
month: 0
firstpage: 3
lastpage: 24
page: 3-24
sections: 
author:
- given: Ohad
  family: Shamir
date: 2013-06-13
address: Princeton, NJ, USA
publisher: PMLR
container-title: Proceedings of the 26th Annual Conference on Learning Theory
volume: '30'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 6
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
