---
pdf: http://proceedings.mlr.press/v30/Zhang13.pdf
title: Divide and Conquer Kernel Ridge Regression
abstract: 'We study a decomposition-based scalable approach to performing kernel ridge
  regression.  The method is simply described: it randomly partitions a dataset of
  size N into m subsets of equal size, computes an independent kernel ridge regression
  estimator for each subset, then averages the local solutions into a global predictor.
  This partitioning leads to a substantial reduction in computation time versus the
  standard approach of performing kernel ridge regression on all N samples. Our main
  theorem establishes that despite the computational speed-up, statistical optimality
  is retained: that so long as m is not too large, the partition-based estimate achieves
  optimal rates of convergence for the full sample size N.  As concrete examples,
  our theory guarantees that m may grow polynomially in N for Sobolev spaces, and
  nearly linearly for finite-rank kernels and Gaussian kernels. We conclude with simulations
  complementing our theoretical results and exhibiting the computational and statistical
  benefits of our approach.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Zhang13
month: 0
tex_title: Divide and Conquer Kernel Ridge Regression
firstpage: 592
lastpage: 617
page: 592-617
sections: 
author:
- given: Yuchen
  family: Zhang
- given: John
  family: Duchi
- given: Martin
  family: Wainwright
date: 2013-06-13
address: Princeton, NJ, USA
publisher: PMLR
container-title: Proceedings of the 26th Annual Conference on Learning Theory
volume: '30'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 6
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
