<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Algorithms and Hardness for Robust Subspace Recovery | COLT 2013 | JMLR W&amp;CP</title>

		<!-- Stylesheet -->
		<link rel="stylesheet" type="text/css" href="../css/jmlr.css">

		<!-- MathJax -->
		<script type="text/x-mathjax-config">
  			MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
		</script>
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>

		<!-- Metadata -->
		<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Algorithms and Hardness for Robust Subspace Recovery">
<meta name="citation_author" content="Hardt, Moritz">
<meta name="citation_author" content="Moitra, Ankur">

<meta name="citation_publication_date" content="2013">
<meta name="citation_conference_title" content="Conference on Learning Theory">
<meta name="citation_firstpage" content="354">
<meta name="citation_lastpage" content="375">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v30/Hardt13.pdf">


    </head>
    <body>

	<div id="fixed">
<a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="../img/jmlr.jpg" align="right" border="0"></a> 
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers
 </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> 
News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> 
Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a>
 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> 
Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open 
Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="../img/RSS.gif" class="rss" alt="RSS Feed">
</a>

</p>
	</div>

	<div id="content">
		<h1>Algorithms and Hardness for Robust Subspace Recovery</h1>

<div id="authors">Moritz Hardt, Ankur Moitra</div>;
<div id="info">JMLR W&amp;CP 30 : 354–375, 2013</div>



<h2>Abstract</h2>
<div id="abstract">
	We consider a fundamental problem in unsupervised learning called subspace recovery: given a collection of m points in <span class="math">\(R^n\)</span>, if many but not necessarily all of these points are contained in a <span class="math">\(d\)</span>-dimensional subspace <span class="math">\(T\)</span> can we find it? The points contained in T are called inliers and the remaining points are outliers. This problem has received considerable attention in computer science and in statistics. Yet efficient algorithms from computer science are not robust to adversarial outliers, and the estimators from robust statistics are hard to compute in high dimensions. This is a serious and persistent issue not just in this application, but for many other problems in unsupervised learning. Are there algorithms for subspace recovery that are both robust to outliers and efficient? We give an algorithm that finds <span class="math">\(T\)</span> when it contains more than a <span class="math">\(d/n\)</span> fraction of the points. Hence, for say <span class="math">\(d = n/2\)</span> this estimator is both easy to compute and well-behaved when there are a constant fraction of outliers. We prove that it is small set expansion hard to find <span class="math">\(T\)</span> when the fraction of errors is any larger and so our estimator is an optimal compromise between efficiency and robustness. In fact, this basic problem has a surprising number of connections to other areas including small set expansion, matroid theory and functional analysis that we make use of here.
</div>

<h2>Related Material</h2>
<div id="extras">
	<ul>
		<li><a href="Hardt13.pdf">Download PDF</a></li>
		
	</ul>
</div>

	</div>

    </body>
</html>
